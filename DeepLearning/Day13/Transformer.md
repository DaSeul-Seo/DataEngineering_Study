- Seq2Seq(RNN) ì™€ ê³µí†µì 
    - Encoderì™€ Decoderë¡œ êµ¬ì„±
    - Decoder ì´í›„ Dense Layerì™€ softmaxë¡œ yë¥¼ êµ¬í•œë‹¤.
- ì°¨ì´ì 
    - RNN ê³„ì—´ì˜ ëª¨ë¸ì´ ì•„ë‹Œ Attentionìœ¼ë¡œë§Œ í•™ìŠµ ì§„í–‰
    - Positional Encodingì„ í†µí•´ RNNì²˜ëŸ¼ ê° í† í°ì— ìˆœì„œ ì •ë³´ë¥¼ ë§¤ê¸´ë‹¤
        - Embedding â†’ Positional Encoding â†’ í•™ìŠµ
    - RNNì€ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµì´ ë˜ì–´ì•¼ í•œë‹¤. â‡’ ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆ ìˆ˜ë¡ ì˜¤ë˜ê±¸ë¦°ë‹¤.
        - ì²« ë²ˆì§¸ í† í° â†’ ë‘ ë²ˆì§¸ í† í° â†’ â€¦
    - TransformerëŠ” ë³‘ë ¬ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ë‹¤.
        - ìœ ì‚¬ë„ ê¸°ë°˜ì´ê¸° ë•Œë¬¸ì—
        - ëª¨ë“  í† í°ì´ ë™ì‹œì— í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤.

### Transformer ì „ì²´

![1](https://github.com/DaSeul-Seo/DataEngineering_Study/assets/67898022/9bca13f7-b09b-4647-912f-0430642f0428)

### Transformer êµ¬ì¡°

1. Input, Output
    1. Input : Embedding + Positional Encoding (Matrix)
    2. Output : 
    
    ![2](https://github.com/DaSeul-Seo/DataEngineering_Study/assets/67898022/cb49e0bd-dd2e-4a1c-aeb5-e8b96f4a7d80)

2. Word Embedding (Encoder, Decoder)
    1. One-Hot Encodingì„ Embedding
    
    ![3](https://github.com/DaSeul-Seo/DataEngineering_Study/assets/67898022/54d71131-56ef-4550-b3ab-4d88a45b2c71)

3. Positional Encoding (Encoder, Decoder)
    1. RNN ì œê±°
    2. Embedding ê²°ê³¼ì— Positional Encoding ê²°ê³¼ë¥¼ í•©ì¹œë‹¤. (ë”í•œë‹¤) â‡’ ìœ„ì¹˜ ì •ë³´ê°€ í¬í•¨ëœ Embedding
    3. ë³‘ë ¬í™” ê°€ëŠ¥í•´ì§
    4. sin(ì§ìˆ˜), cos(í™€ìˆ˜) ì‚¬ìš©í•´ ìœ„ì¹˜ ì •ë³´ ë§¤ê¸´ë‹¤.
    
    ![4](https://github.com/DaSeul-Seo/DataEngineering_Study/assets/67898022/f99a4298-7133-4296-8dae-ef4fe02dd826)

4. Self-Attention (ì‹œê°„ ë‹¨ì¶•) (Encoder, Decoder)
    1. ì¼ë°˜ì ì¸ Attentionì€ Keysê°€ Encoderì—, QueryëŠ” Decoderì— ìˆë‹¤.
    2. TransformerëŠ” Encoderë¼ë¦¬, Decoderë¼ë¦¬ Attentionì„ í•œë‹¤. (Self-Attention)
    3. ìê¸°ì™€ ìê¸° ìì‹ ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°
    
    ![5](https://github.com/DaSeul-Seo/DataEngineering_Study/assets/67898022/99d79e92-5dfa-4fb0-b62c-f7023e3c54f5)

5. QKV
    1. Query (I)
        1. í•˜ë‚˜ì˜ Input(Embedding + Positional Encoding
    2. Keys (I, an, a, student)
        1. ì „ì²´ í† í°
        2. Embedding + Positional Encoding
    3. Value
        1. Keysì— ê°ê° Queryì™€ Keysì˜ ìœ ì‚¬ë„ì˜ ê°’ì„ ê³±í•´ì„œ ë”í•œ ê°’
        2. Iì— ëŒ€í•œ Queryì™€ Keysì˜ ìœ ì‚¬ë„ ê°’
        3. Keysì— ê°ê° Iì— ëŒ€í•œ ìœ ì‚¬ë„ ê°’ì„ ê³±í•œë‹¤.
            1. I * 0.5 / am * 0.4 / a * 0.05 / student * 0.05
        4. ê·¸ ê°’ë“¤ì„ ë‹¤ ë”í•œë‹¤. â‡’ Iì— ëŒ€í•œ í•™ìŠµ ê²°ê³¼
6. Multi-Head Self Attention (Encoding)
    1. ê° Queryë¥¼ ë³‘ë ¬ë¡œ ê°ê° ì²˜ë¦¬í•œë‹¤.
    2. ì†ë„ê°€ ë¹ ë¥´ë‹¤
    
    ![6](https://github.com/DaSeul-Seo/DataEngineering_Study/assets/67898022/2a6cbbe1-c9a8-4fad-aec5-ab3075b720f8)

7. Masking Multi-Head Self Attention (Decoding)
    1. íŠ¹ì • ë°ì´í„°ë¥¼ ê°€ë¦°ë‹¤.
    2. ì™œ Decoderì—ë§Œ ìˆëŠ”ê°€? (inputì´ targetì´ë¼ì„œ)
        1. Encodingì€ feature, Decoderì—ì„œ targetì´ Input Dataì´ë‹¤.
        2. Decoderì—ì„œ ì˜ˆì¸¡í•  ë°ì´í„°ë¥¼ ì „ë¶€ë‹¤ Inputìœ¼ë¡œ ë„£ì–´ë²„ë¦¬ë©´ ëª¨ë¸ì´ í•™ìŠµì„ í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì¼ë¶€ ë°ì´í„°ë¥¼ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬ í›„ Input Dataë¡œ ì‚¬ìš©
        3. ì •ë‹µì„ ì•Œê³  í•™ìŠµì„ í•˜ê¸° ë•Œë¬¸ì— Maskingì´ í•„ìš”
    3. Seq2Seqì—ì„œëŠ” Maskingì„ ì•ˆí•˜ëŠ” ì´ìœ ?
        1.  RNN íŠ¹ì„±ìƒ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— í•´ë‹¹ ìˆœì„œì—ëŠ” ë‹¤ìŒì— ì˜¤ëŠ” ë‹¨ì–´ë¥¼ ëª¨ë¥¸ë‹¤. â‡’ target ë°ì´í„°ë¥¼ ëª¨ë¥¸ë‹¤.
    
    ![7](https://github.com/DaSeul-Seo/DataEngineering_Study/assets/67898022/ccf7aa65-a299-40e3-9d3a-c6c239a0d49c)

8. Feed-Forward (ì„ í˜•íšŒê·€ëª¨ë¸) (Encoder, Decoder)
    1. Fully-Connected Layer
    2. ReLU
    3. Fully-Connected Layer
    
    ![8](https://github.com/DaSeul-Seo/DataEngineering_Study/assets/67898022/9aba988b-8197-4983-ab8c-4940737ea38a)

9. Add & Normalization (Encoder, Decoder)
    1. ResNet ê¸°ë²• ì‚¬ìš©
    2. Masked Multi-Head Attention ê²°ê³¼ê°’ê³¼ input í•©ì¹œë‹¤. (Decoder)
    3. Multi-Head Attention ê²°ê³¼ê°’ê³¼ í•©ì¹œë‹¤.
    4. Feed Forward ê²°ê³¼ê°’ê³¼ í•©ì¹œë‹¤.
    
    ![9](https://github.com/DaSeul-Seo/DataEngineering_Study/assets/67898022/eefeede5-0236-423d-85d7-424bf141461c)

10. Output Softmax
    - Encoderì˜ Keys, Valueì˜ ê°’ì´ Decoderì— ë“¤ì–´ê°„ë‹¤.
    - 1, 2 Attentionì€ ìê¸° ìì‹ ì— ëŒ€í•œ ìœ ì‚¬ë„ ê³„ì‚°
    - 3 Attentionì€ Encoderì˜ output(Keys, Values), Decoder ì˜ inputì— ëŒ€í•œ ìœ ì‚¬ë„ ê³„ì‚°
    
    ![10](https://github.com/DaSeul-Seo/DataEngineering_Study/assets/67898022/ea74ed86-d5f0-4c06-ba09-7a0125a1c0d3)

### Transformer Code

![11](https://github.com/DaSeul-Seo/DataEngineering_Study/assets/67898022/facce55a-3118-47a4-9845-c229d083bbe9)

- nn.Transformer
- Parameter
    - `d_model`: íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¸ì½”ë”ì™€ ë””ì½”ë”ì—ì„œì˜ ì •í•´ì§„ ì…ë ¥ê³¼ ì¶œë ¥ì˜ í¬ê¸°ë¥¼ ì˜ë¯¸(default=512)
    - `num_encoder_layers`: íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì—ì„œ ì¸ì½”ë”ê°€ ì´ ëª‡ ì¸µìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆëŠ”ì§€ë¥¼ ì˜ë¯¸(default=6)
    - `num_decoder_layers`: íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì—ì„œ ë””ì½”ë”ê°€ ì´ ëª‡ ì¸µìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆëŠ”ì§€ë¥¼ ì˜ë¯¸(default=6)
    - `nhead`: ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ëª¨ë¸ì˜ í—¤ë“œ ìˆ˜, ì–´í…ì…˜ì„ ì‚¬ìš©í•  ë•Œ ì—¬ëŸ¬ ê°œë¡œ ë¶„í• í•´ì„œ ë³‘ë ¬ë¡œ ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ê°’ì„ ë‹¤ì‹œ í•˜ë‚˜ë¡œ í•©ì¹˜ëŠ” ë°©ì‹ì—ì„œ ë³‘ë ¬ì˜ ìˆ˜ (default=8)
    - `dim_feedforward`: feedforward network model ì˜ ì°¨ì›, í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µì˜ í¬ê¸°(default=2048).
- íŒ¨í‚¤ì§€ ì„¤ì¹˜

```python
!pip install kiwipiepy
```

- Data ì—°ê²°

```python
from google.colab import drive
drive.mount('/content/drive')
```

```python
DATA_PATH = "/content/data/MyDrive/Playdata/06.Deep Learning/Datas/"
SEED = 42

device = 'cuda' if torch.cuda.is_available() else 'cpu'
device
```

- ëª¨ë“ˆ Import

```python
import pandas as pd
import numpy as np
import torch
from tqdm.auto import tqdm
import random
import os
```

- Seed í•¨ìˆ˜

```python
def reset_seeds(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
```

### ì˜ˆì œ - ë¶„ë¥˜ ë¬¸ì œ

- Data Load

```python
df_ko = pd.read_csv(DATA_PATH+"naver_review/naver_review_train.csv", sep="\t")

df_ko = df_ko[:10000]
print(f'{df_ko.isnull().sum().sum()} / {df_ko.shape}')
df_ko.head()
```

- Text Preprocessing
    - Cleaning
        
        ```python
        df_ko['document'] = df_ko['document'].map(lambda x: x.strip()) # ë¬¸ìì—´ì˜ ì•ë’¤ ê³µë°± ì œê±°
        df_ko['document'] = df_ko['document'].map(lambda x: x.replace("..", " "))
        
        print(f'{df_ko.isnull().sum().sum()} / {df_ko.shape}')
        df_ko.head()
        ```
        
    - Tokenize & Voca
        
        ```python
        from kiwipiepy import Kiwi
        from kiwipiepy.utils import Stopwords
        
        stopwords =  Stopwords() # ë¶ˆìš©ì–´
        kiwi = Kiwi() # í˜•íƒœì†Œ ë¶„ì„ê¸°
        ```
        
        ```python
        def tokenizer(text):
            tokens = kiwi.tokenize(text, stopwords=stopwords) # í† í°í™”
            return [ t.form for t in tokens if t.tag[0] in "NJMV" ] # N:ëª…ì‚¬, J:ì¡°ì‚¬, M:ê´€í˜•ì‚¬/ë¶€ì‚¬, V: ë™ì‚¬
        
        def yield_tokens(data, tokenizer):
            for text in tqdm(data):
                yield tokenizer(text)
        ```
        
        ```python
        from torchtext.vocab import build_vocab_from_iterator
        
        gen = yield_tokens(df_ko["document"],tokenizer) # í† í°í™”
        vocab = build_vocab_from_iterator(gen, specials=["<pad>","<unk>"]) # ì–´íœ˜ì§‘
        vocab.set_default_index(vocab["<unk>"])
        
        len(vocab)
        ```
        
        ```python
        features = [ vocab(tokenizer(text)) for text in tqdm(df_ko["document"].tolist()) ]
        len(features)
        ```
        
    - Padding
        
        ```python
        # ìµœëŒ€ í† í° ìˆ˜
        max_len = max(len(lst) for lst in features)
        
        # ìµœëŒ€ í† í° ìˆ˜ ë§Œí¼ ë‚˜ë¨¸ì§€ í† í°ì„ ìœ¼ë¡œ ì±„ìš´ë‹¤
        features = [ lst + [0] * (max_len - len(lst))  if len(lst) < max_len else lst for lst in tqdm(features) ]
        features = np.array(features)
        features.shape
        
        # featureì™€ shapeì„ ë§ì¶”ê¸° ìœ„í•´
        target = df_ko["label"].to_numpy().reshape(-1,1)
        target.shape
        ```
        
- Dataset

```python
from torch.utils.data import Dataset

class ReviewDataset(Dataset):
    def __init__(self,features,target=None):
        super().__init__()
        self.features = features # features
        self.target = target # target

    def __len__(self):
        return self.features.shape[0]

    def __getitem__(self,idx):
        item = {}
        item["x"] = torch.LongTensor(self.features[idx])
        if self.target is not None:
            item["y"] = torch.Tensor(self.target[idx])
        return item

dt = ReviewDataset(features,target)
len(dt)
```

- DataLoader

```python
from torch.utils.data import DataLoader

dl = DataLoader(dt,batch_size=2)
batch = next(iter(dl))
batch # batch, token
```

- Model

```python
emb = torch.nn.Embedding(len(vocab),512)
x = emb(batch["x"])
x.shape # batch, token, toeknì„ ì„¤ëª…í•´ì£¼ëŠ” embedding ì°¨ì›
```

```python
enc_layer = torch.nn.TransformerEncoderLayer(512,8,batch_first=True)
encoder = torch.nn.TransformerEncoder(enc_layer,2)
encoder(x).shape # batch, token, toeknì„ ì„¤ëª…í•´ì£¼ëŠ” embedding ì°¨ì›
```

```python
class Net(torch.nn.Module):
    def __init__(self,vocab_size,max_len,d_model=32,nhead=4,dim_feedforward=64,num_layers=1,device="cpu"):
        super().__init__()
        self.pos = torch.arange(max_len).to(device)
        self.pos_emb_layer = torch.nn.Embedding(max_len, d_model)

        self.emb_layer = torch.nn.Embedding(vocab_size, d_model)
        self.enc_layer = torch.nn.TransformerEncoderLayer(d_model,nhead,dim_feedforward,batch_first=True)
        self.encoder = torch.nn.TransformerEncoder(self.enc_layer,num_layers)
        self.flatten = torch.nn.Flatten()
        self.dropout = torch.nn.Dropout(0.5)
        self.output_layer = torch.nn.Linear(max_len*d_model ,1)
    def forward(self,x):
        # positional Encoding
        pos = self.pos_emb_layer(self.pos) # s , f
        # embedding
        x = self.emb_layer(x) # b, s ,f
        # positional Encoding + embedding
        x = torch.add(x,pos) # b ,s ,f

        # encoder
        x = self.encoder(x) # b, s, f
        # flatten : ì„ í˜•ëª¨ë¸ì— ë„£ì–´ì•¼ í•˜ë‹ˆê¹Œ
        x = self.flatten(x) # b , sxf (2ì°¨ì› í˜•íƒœ)
        x = self.dropout(x)
        return self.output_layer(x)
```

```python
model = Net(len(vocab), max_len)
pred = model(batch["x"])
pred.shape
```

- Engine

```python
# train
def train_loop(dataloader,model,loss_fn,optimizer,device):
    epoch_loss = 0
    model.train() # ëª¨ë¸ ê°ì²´ë¥¼ í•™ìŠµëª¨ë“œë¡œ ì „í™˜
    for batch in dataloader:
        pred = model( batch["x"].to(device) )
        loss = loss_fn( pred,batch["y"].to(device) )

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    epoch_loss /= len(dataloader)

    return epoch_loss
```

```python
# test
@torch.no_grad()
def test_loop(dataloader,model,loss_fn,device):
    model.eval() # í‰ê°€ ëª¨ë“œ
    sig = torch.nn.Sigmoid()
    pred_list = []
    epoch_loss = 0
    for batch in dataloader:
        pred = model(batch["x"].to(device))

        # ê²€ì¦ í‰ê°€í•  ê²½ìš°
        if batch.get("y") is not None:
            loss = loss_fn(pred,batch["y"].to(device))
            epoch_loss += loss.item()

        # ì˜ˆì¸¡ê°’ ë§Œë“¤ê¸°
        pred = sig(pred)
        pred = pred.to("cpu").numpy()
        pred_list.append(pred)

    pred = np.concatenate(pred_list)
    epoch_loss /= len(dataloader)
    return epoch_loss, pred
```

- Training

```python
batch_size = 32
loss_fn = torch.nn.BCEWithLogitsLoss() # ì†ì‹¤ ê°ì²´
epochs = 100
n_splits = 5 # í´ë“œìˆ˜
model_hp = {
    "vocab_size": len(vocab),
    "max_len": max_len,
    "d_model": 32,
    "nhead" : 8,
    "dim_feedforward" : 64,
    "num_layers": 1,
    "device" : device
}
```

```python
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
cv = KFold(n_splits=n_splits,shuffle=True, random_state=SEED)
```

```python
is_holdout = False
reset_seeds(SEED)
best_score_list = []
for i,(tri,vai) in enumerate(cv.split(features)):

    model = Net(**model_hp).to(device)
    optimizer = torch.optim.Adam(model.parameters())

    train_dt = ReviewDataset(features[tri],target[tri])
    valid_dt = ReviewDataset(features[vai],target[vai])
    train_dl = torch.utils.data.DataLoader(train_dt, batch_size=batch_size, shuffle=True)
    valid_dl = torch.utils.data.DataLoader(valid_dt, batch_size=batch_size,shuffle=False)

    best_score = 0
    patience = 0

    for epoch in tqdm(range(epochs)):

        train_loss = train_loop(train_dl, model, loss_fn,optimizer,device )
        valid_loss , pred = test_loop(valid_dl, model, loss_fn,device  )
        pred = np.where(pred>0.5,1,0)

        score = accuracy_score(target[vai],pred)
        print(train_loss,valid_loss, score)
        patience += 1
        if best_score < score:
            patience = 0
            best_score = score
            torch.save(model.state_dict(),f"model_{i}.pth")

        if patience == 5:
            break
    print(f"Fold ({i}), BEST ACC: {best_score}")
    best_score_list.append(best_score)

    if is_holdout:
        break
```

### ì˜ˆì œ - ë²ˆì—­

- Data Load

```python
train = pd.read_csv(f"{DATA_PATH}translate_en_ko.csv")
train.shape
```

- Text Preprocessing (í•œêµ­ì–´)
    - Cleaning
        
        ```python
        train["en"] = train["en"].str.replace("[^a-zA-Z0-9 .,!?\'\"]" , "",regex=True).str.lower()
        train["ko"] = train["ko"].str.replace("[^ê°€-í£0-9 .,!?\'\"]" , "",regex=True)
        ```
        
    - Tokenize & Voca
        
        ```python
        from kiwipiepy import Kiwi
        from kiwipiepy.utils import Stopwords
        
        stopwords =  Stopwords() # ë¶ˆìš©ì–´
        kiwi = Kiwi() # í˜•íƒœì†Œ ë¶„ì„ê¸°
        ```
        
        ```python
        def tokenizer(text):
            tokens = kiwi.tokenize(text, stopwords=stopwords) # í† í°í™”
            return [ t.form for t in tokens if t.tag[0] in "NJMV" ] # N:ëª…ì‚¬, J:ì¡°ì‚¬, M:ê´€í˜•ì‚¬/ë¶€ì‚¬, V: ë™ì‚¬
        
        def yield_tokens(data, tokenizer):
            for text in tqdm(data):
                yield tokenizer(text)
        ```
        
        ```python
        from torchtext.vocab import build_vocab_from_iterator
        
        gen = yield_tokens(train["ko"],tokenizer) # í† í°í™”
        vocab_ko = build_vocab_from_iterator(gen, specials=["<pad>","<unk>"]) # ì–´íœ˜ì§‘
        vocab_ko.set_default_index(vocab_ko["<unk>"])
        len(vocab_ko)
        ```
        
        ```python
        src_data = [ vocab_ko(tokenizer(text)) for text in train["ko"].tolist() ]
        ```
        
- Text Preprocessing (ì˜ì–´)
    - Tokenize & Voca
        
        ```python
        from torchtext.data.utils import get_tokenizer
        tokenizer = get_tokenizer("basic_english")
        ```
        
        ```python
        tgt_data = []
        for text in train["en"]:
            t_list = ["<sos>"] + tokenizer(text) + ["<eos>"]
            tgt_data.append(t_list)
        ```
        
        ```python
        vocab_en = build_vocab_from_iterator(tgt_data,specials=["<pad>","<unk>","<sos>","<eos>"])
        vocab_en.set_default_index(vocab_en["<unk>"])
        len(vocab_en)
        ```
        
        ```python
        tgt_data = [ vocab_en(lst) for lst in tgt_data ]
        ```
        
- Dataset

```python
from torch.utils.data import Dataset

class TranslateDataset(torch.utils.data.Dataset):
    def __init__(self,src,tgt):
        self.src = src
        self.tgt = tgt
    def __len__(self):
        return len(self.src)
    def __getitem__(self,idx):
        item = {}
        item["src"] = torch.LongTensor(self.src[idx])
        item["tgt"] = torch.LongTensor(self.tgt[idx])
        return item

dt = TranslateDataset(src_data,tgt_data)
dt[0]
```

- collate_fn

```python
def collate_fn(lst):
    src= [ item["src"] for item in lst ] # Encoder input data
    tgt= [ item["tgt"] for item in lst ] # Decoder input data(target)
    src = torch.nn.utils.rnn.pad_sequence(src,batch_first=True)
    tgt = torch.nn.utils.rnn.pad_sequence(tgt,batch_first=True)
    return {"src": src, "tgt": tgt}
```

- DataLoader

```python
from torch.utils.data import DataLoader

dl = DataLoader(dt,batch_size=2,collate_fn=collate_fn)
batch = next(iter(dl))
batch
```

- Model
    - forward ì£¼ìš” íŒŒë¼ë¯¸í„°
        - src : ì¸ì½”ë”ì˜ ì…ë ¥ í…ì„œ
        - tgt : ë””ì½”ë”ì˜ ì…ë ¥ í…ì„œ
        - tgt_mask : ì˜ˆì¸¡ì‹œì ì˜ ì–´í…ì…˜ ë°©ì§€ë¥¼ ìœ„í•œ tgt ì‹œí€€ìŠ¤ì— ëŒ€í•œ ë§ˆìŠ¤í¬

```python
transformer = torch.nn.Transformer(batch_first=True)
transformer
```

```python
d_model = 512
src_emb = torch.nn.Embedding(len(vocab_ko), d_model ) # Encoder input data Embedding
tgt_emb = torch.nn.Embedding(len(vocab_en), d_model ) # Decoder input data Embedding(target)

src = src_emb(batch["src"])
tgt = tgt_emb(batch["tgt"])
src.shape , tgt.shape
```

```python
# Decoder Input data Masking
tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(tgt.shape[1])
tgt_mask
```

```python
# (Encoder input data, Decoder masking input data)
output = transformer(src,tgt , tgt_mask = tgt_mask)
output.shape
```

- Positional Encoding

```python
class PositionalEncoding(torch.nn.Module):
    def __init__(self,max_len,d_model):
        super().__init__()
        pos_encoding = torch.zeros(max_len,d_model) # seq, feature
        pos = torch.arange(0,max_len,dtype=torch.float32).view(-1,1)
        _2i= torch.arange(0,d_model,step=2,dtype=torch.float32)

        pos_encoding[:,0::2] = torch.sin(pos / 10000 ** (_2i/d_model)) # ì§ìˆ˜
        pos_encoding[:,1::2] = torch.cos(pos / 10000 ** (_2i/d_model)) # í™€ìˆ˜
        # pos_encoding -> (max_len, d_model)
        pos_encoding = pos_encoding.unsqueeze(0) # (batch, max_len, d_model)

        self.register_buffer("pos_encoding",pos_encoding)
    def forward(self,x):
        return x + self.pos_encoding[:,:x.shape[1]]

pe = PositionalEncoding(1000,512)
pe(src).shape
```

- Seq2Seq

```python
class Net(torch.nn.Module):
    def __init__(self,
                 src_vocab_size,
                 tgt_vocab_size,
                 max_len=1000,
                 d_model = 512,
                 nhead=8,
                 num_encoder_layers=6,
                 num_decoder_layers=6,
                 dim_feedforward=2048):

        super().__init__()
        # Encoder Embedding (input data)
        self.src_emb = torch.nn.Embedding(src_vocab_size,d_model)
        # Decoder Embedding (target input date)
        self.tgt_emb = torch.nn.Embedding(tgt_vocab_size,d_model)

        # Positional Encoding
        self.pe = PositionalEncoding(max_len,d_model)

        # Transformer
        self.transformer = torch.nn.Transformer(d_model=d_model,
                                                nhead=nhead,
                                                num_encoder_layers=num_encoder_layers,
                                                num_decoder_layers=num_decoder_layers,
                                                dim_feedforward=dim_feedforward,
                                                batch_first=True)
        # ì„ í˜•
        self.fc_layer = torch.nn.Linear(d_model,tgt_vocab_size)

    def forward(self,src,tgt,tgt_mask):
        src = self.pe( self.src_emb(src) ) # Encoder
        tgt = self.pe( self.tgt_emb(tgt) ) # Decoder
        x = self.transformer(src,tgt,tgt_mask=tgt_mask) # batch,seq,d_model
        return self.fc_layer(x) # batch,seq,ë‹¨ì–´ë³„ ì˜ˆì¸¡ê°’

    # ì‹¤ì œ ì˜ˆì¸¡ì‹œ ì‚¬ìš©í•  ë©”ì„œë“œ
    def ecoder(self,src):
        src = self.pe( self.src_emb(src) )
        return self.transformer.encoder(src)

    def decoder(self,tgt,memory):
        tgt = self.pe( self.tgt_emb(tgt) )
        x = self.transformer.decoder(tgt,memory=memory)
        return self.fc_layer(x)
```

```python
tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(batch["tgt"].shape[1])
tgt_mask
```

```python
model = Net(len(vocab_ko), len(vocab_en))
pred = model(batch["src"],batch["tgt"],tgt_mask)
pred.shape
```

```python
src = batch["src"][:1]
src # bathc, seq

tgt = batch["tgt"][:1,:1]
tgt # batch , seq
```

```python
model.eval()
memory = model.ecoder(src)
pred = model.decoder(tgt,memory)
pred.shape
```

```python
pred[-1,-1].argmax()
vocab_en.lookup_token(1974)
```

- Engine

```python
# EarlyStopper
class EarlyStopper(object):

    def __init__(self, num_trials, save_path):
        self.num_trials = num_trials
        self.trial_counter = 0
        self.best_loss = np.inf
        self.save_path = save_path

    def is_continuable(self, model, loss):
        if loss < self.best_loss:
            self.best_loss = loss
            self.trial_counter = 0 # ì´ˆê¸°í™”
            torch.save(model, self.save_path)
            return True
        elif self.trial_counter + 1 < self.num_trials:
            self.trial_counter += 1 # ê¸°ì¡´ ì‹œë„íšŸìˆ˜ + 1
            return True
        else:
            return False
```

```python
# train
def train_loop(dataloader,model,loss_fn,optimizer,device):
    epoch_loss = 0
    model.train()

    for batch in dataloader:
        src = batch["src"].to(device)
        tgt = batch["tgt"].to(device)
        tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(tgt.shape[1]).to(device)
        pred = model(src,tgt,tgt_mask) # batch, seq, ë‹¨ì–´ë³„ ì˜ˆì¸¡ì‹¤ìˆ˜ê°’
        nclass = pred.shape[2]
        pred = pred[:,:-1].reshape(-1,nclass) # ì‹œì ë³„ ì˜ˆì¸¡ ì‹¤ìˆ˜ê°’ë“¤ì´ ë¨
        tgt = tgt[:,1:].reshape(-1) # ë‹¤ì¤‘ë¶„ë¥˜ ë¬¸ì œì—ì„œëŠ” ì •ë‹µë°ì´í„°ëŠ” 1ì°¨ì› ë²¡í„°í˜•íƒœì—¬ì•¼í•¨.

        mask = tgt > 2
        tgt = tgt[mask]
        pred = pred[mask]
        loss = loss_fn(pred,tgt)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    epoch_loss /= len(dataloader)
    return epoch_loss
```

- Training

```python
src_vocab_size, tgt_vocab_size = len(vocab_ko) , len(vocab_en)
batch_size = 64
loss_fn = torch.nn.CrossEntropyLoss()
epochs = 500
```

```python
reset_seeds(SEED)

model = Net(src_vocab_size, tgt_vocab_size).to(device)
optimizer = torch.optim.Adam(model.parameters())
early_stopper = EarlyStopper(num_trials=10, save_path=f'best_model.pt')

train_dt = TranslateDataset(src_data,tgt_data)
train_dl = torch.utils.data.DataLoader(train_dt,batch_size=batch_size,shuffle=True,collate_fn=collate_fn)

train_loss_list = []
for i in tqdm(range(epochs)):
    train_loss = train_loop(train_dl,model,loss_fn,optimizer,device)
    # print(train_loss)
    train_loss_list.append(train_loss)
    if not early_stopper.is_continuable(model, train_loss):
        print(f'epoch:{i} >> best loss: {early_stopper.best_loss}')
        break
```

```python
import seaborn as sns
import matplotlib.pyplot as plt

sns.lineplot(train_loss_list)
```

<aside>
ğŸ’¡ Reference

</aside>

- Transformer
    - https://gaussian37.github.io/dl-concept-transformer/
- êµ¬ì¡°
    - [https://glanceyes.com/entry/Transformerì˜-Multi-Head-Attentionê³¼-Transformerì—ì„œ-ì“°ì¸-ë‹¤ì–‘í•œ-ê¸°ë²•#toc-link-1](https://glanceyes.com/entry/Transformer%EC%9D%98-Multi-Head-Attention%EA%B3%BC-Transformer%EC%97%90%EC%84%9C-%EC%93%B0%EC%9D%B8-%EB%8B%A4%EC%96%91%ED%95%9C-%EA%B8%B0%EB%B2%95#toc-link-1)
