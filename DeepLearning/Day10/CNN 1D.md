<aside>
ğŸ’¡ Design Logic

</aside>

### CNN 2D

1. Load Data
2. EDA
    1. ì´ë¯¸ì§€(ì»¬ëŸ¬ / í‘ë°±)
3. Dataset Class
    1. Transform
4. DataLoader
5. Modeling
    1. Train Loop
    2. Test Loop

### NLP

1. Load Data
2. EDA
    1. ê¸ì •/ë¶€ì •
    2. ëŒ€ì†Œë¬¸ìí™•ì¸
3. Text Preprocessing
    1. Cleaning
        1. ê³µë°±ì œê±°
        2. íŠ¹ìˆ˜ë¬¸ì ì œê±°
    2. Tokenization
        1. ê¸€ì ìª¼ê°œê¸°(í† í°í™”)
    3. Embedding â‡’ ê¸€ì(í† í°)ì„ vector(ìˆ«ì)í™”
4. Dataset Class
5. DataLoader
6. Modeling

---

### CNN 1D (Convolutional Neural Network 1D)

- ì—°ì‚°ì˜ ê²°ê³¼ê°€ vector
    - in_channels : input ì˜ feature ì°¨ì›ìˆ˜
    - out_channels: output ì˜ feature ì°¨ì›ìˆ˜
    - kernel_size : ì…ë ¥ê¸¸ì´ë¥¼ ì–¼ë§ˆë§Œí¼ ë³¼ê²ƒì¸ê°€
    - stride: kernel ì„ ì–¼ë§ˆë§Œí¼ì”© ì´ë™í•  ê²ƒì¸ê°€
    - padding: ì–‘ë°©í–¥ìœ¼ë¡œ ì–¼ë§ˆë§Œí¼ íŒ¨ë”©í•  ê²ƒì¸ê°€
    - input tensor shape
        - batch, feature dim, time step (ì…ë ¥ê¸¸ì´)

### CNN Code

1. import & Module
    
    ```python
    # ì• ë„ë¦¬ìŠ¤íŠ¸ ëª¨ë“ˆ : ë¶„ì„ ëª¨ë“ˆ
    import numpy as np
    import pandas as pd
    
    # ë”¥ëŸ¬ë‹ ëª¨ë“ˆ
    import torch
    
    # nlp ë”¥ëŸ¬ë‹ ëª¨ë“ˆ
    # í† í°(iterator ê°ì²´)ìœ¼ë¡œë¶€í„° ë‹¨ì–´(vocab)ë¥¼ ë§Œë“œëŠ” í•¨ìˆ˜
    from torchtext.vocab import build_vocab_from_iterator
    
    from tqdm.auto import tqdm
    ```
    
    ```python
    # êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²°(ë°ì´í„° ë¡œë“œë¥¼ ìœ„í•´ì„œ)
    try:
        from google.colab import drive
    
        drive.mount('/content/data')
        DATA_PATH = "/content/data/MyDrive/Playdata/06.Deep Learning/Datas/"
    except:
        DATA_PATH = "./data/"
    ```
    
    ```python
    df_ko = pd.read_csv(DATA_PATH+"naver_review/naver_review_train.csv", sep="\t")
    
    # í…ìŠ¤íŠ¸ ê°ì •ë¶„ì„ NLU
    df_ko = df_ko[:10000]
    print(f'{df_ko.isnull().sum().sum()} / {df_ko.shape}')
    df_ko.head()
    ```
    
2. ê²°ì¸¡ì¹˜ í™•ì¸
    
    ```python
    # 1. ê²°ì¸¡ì¹˜ ë°ì´í„° í™•ì¸ !!
    df_ko.isnull().sum()
    
    print(f'{df_ko.isnull().sum().sum()} / {df_ko.shape}')
    ```
    
    ```python
    # 2. ê²°ì¸¡ì¹˜ ë°ì´í„° ì¡°ê±´ ìƒì„±
    c = df_ko['document'].isnull()
    # 3. ê²°ì¸¡ì¹˜ ì•„ë‹Œ ì¡°ê±´ ì ìš©
    df_tmp = df_ko.loc[c]
    # 4. ê²€ì¦
    print(f'{df_tmp.isnull().sum().sum()} / {df_tmp.shape}')
    df_tmp.head()
    
    print(df_ko.shape)
    print(df_ko.dropna().shape)   #.reset_index(drop=False)
    ```
    
    ```python
    df_ko = df_ko.dropna().reset_index(drop=True) # ê²°ì¸¡ì¹˜ ì œê±°
    
    print(f'{df_ko.isnull().sum().sum()} / {df_ko.shape}')
    df_ko.head()
    ```
    
3. Text Preprocessing
    1. Cleaning
        
        ```python
        # ë¬´ì¡°ê±´ ê³µë°±ì œê±° í•„ìˆ˜(ë¬¸ì¥ì˜ ì•ë’¤)
        df_ko['document'] = df_ko['document'].map(lambda x: x.strip()) # ë¬¸ìì—´ì˜ ì•ë’¤ ê³µë°± ì œê±°
        # íŠ¹ìˆ˜ë¬¸ì(..) ì œê±°
        df_ko['document'] = df_ko['document'].map(lambda x: x.replace("..", " "))
        ```
        
    2. Tokenization
        
        ```python
        !pip install kiwipiepy
        ```
        
        ```python
        from kiwipiepy import Kiwi
        from kiwipiepy.utils import Stopwords
        
        stopwords =  Stopwords() # ë¶ˆìš©ì–´
        kiwi = Kiwi() # í˜•íƒœì†Œ ë¶„ì„ê¸°
        
        result = kiwi.tokenize(text, stopwords=stopwords)
        result
        ```
        
    3. Stemming / Stopword
        
        ```python
        # ê° review data(txet) í† í°í™”í•´ì„œ NJMV ë½‘ì•„ì˜¨ë‹¤.
        def tokenizer(text):
            tokens = kiwi.tokenize(text, stopwords=stopwords) # í† í°í™”
            return [ t.form for t in tokens if t.tag[0] in "NJMV" ] # N:ëª…ì‚¬, J:ì¡°ì‚¬, M:ê´€í˜•ì‚¬/ë¶€ì‚¬, V: ë™ì‚¬
        ```
        
        ```python
        # iterator í•¨ìˆ˜ : forë¬¸ì— yield ë„£ì–´ì£¼ë©´ iteratorê°€ ëœë‹¤.
        # (review data, í˜•íƒœì†Œ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤)
        def yield_tokens(data, tokenizer):
            # dataì— ìˆëŠ” textë¥¼ ê°ê° tokenizer í•¨ìˆ˜ ì ìš©
            for text in tqdm(data):
                # textë¥¼ ê°ê° tokenizer ì ìš©
                yield tokenizer(text)
        ```
        
    4. Vocabulary
        
        ```python
        # iterator ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        gen = yield_tokens(df_ko["document"],tokenizer) # í† í°í™”
        
        # build_vocab_from_iterator
        # iterator ì¸ìŠ¤í„´ìŠ¤ë¥¼ í†µí•´ vocabë¥¼ ë§Œë“ ë‹¤.
        # specials : í•™ìŠµì— í•„ìš”í•œ íŠ¹ë³€í•œ í† í°ì„ ì¶”ê°€í•œë‹¤
        # <pad> : ê° tokenì˜ ê¸¸ì´ë¥¼ í†µì¼ -> ëª¨ë¸ input size ë™ì¼í•˜ê²Œ í•˜ë ¤ê³ 
        # <unk> : ë‚˜ì˜ ë°ì´í„°ì— ì—†ëŠ” ë‹¨ì–´/ë™ì‚¬ì™€ ê°™ì€ tokenì„ í‘œí˜„í•˜ê¸° ìœ„í•œ token ê°’
        # ì•Œìˆ˜ ì—†ëŠ” ë‹¨ì–´(ë‚´ê°€ ê°€ì§€ê³  ìˆì§€ ì•Šì€ ë‹¨ì–´)ê°€ ë“¤ì–´ì˜¤ë©´ unknownìœ¼ë¡œ ì„¤ì •
        vocab = build_vocab_from_iterator(gen, specials=["<pad>","<unk>"]) # ì–´íœ˜ì§‘
        
        # ëª¨ë¥´ëŠ” ë‹¨ì–´/í† í°ì´ ë“¤ì–´ì˜¤ë©´ <unk>í† í°ìœ¼ë¡œ ì¬ì •ì˜
        vocab.set_default_index(vocab["<unk>"])
        
        # ì „ì²´ vocabì´ ê°–ê³  ìˆëŠ” í† í° ìˆ˜
        len(vocab)
        ```
        
        ```python
        # ë„¤ìš”, karnsëŠ” ì…ë ¥ë˜ì–´ ìˆì§€ ì•Šì€ ë‹¨ì–´ì´ë¯€ë¡œ <unk> ê°’ì´ë¼ 1 (specialê°’)
        vocab(['ì§œì¦', 'ì§œì¦', 'ë‚˜', 'ë„¤ìš”', 'ëª©ì†Œë¦¬', 'karns'])
        ```
        
    5. í† í°í™”
        
        ```python
        features = []
        
        # review dataë¥¼ í•˜ë‚˜ì”© ê°€ì ¸ì˜¤ê¸°
        for review_text in tqdm(df_ko["document"].tolist()):
          # í•˜ë‚˜ì˜ review dataë¥¼ í† í°í™”
          tokens = tokenizer(review_text)
          # í† í°í™”ëœ vocabì„ í†µí•´ ìˆ«ìí™”(index)
          token_indexes = vocab(tokens)
          # feature ë¦¬ìŠ¤íŠ¸ì— ë‹´ê¸°
          features.append(token_indexes)
          break
        ```
        
        ```python
        # matrix -> ì „ì²´ ë°ì´í„°ë¥¼ í† í°í™”í•œ í›„, vocabì„ ì´ìš©í•´ index ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        features = [ vocab(tokenizer(text)) for text in tqdm(df_ko["document"].tolist()) ]
        
        len(features)
        features
        ```
        
    6. Padding
        
        ```python
        # í† í° ìµœëŒ€ ê¸¸ì´
        max_len = max(len(lst) for lst in features)
        max_len
        ```
        
        ```python
        # paddingì´ ì ìš©ëœ ë°ì´í„° ë‹´ëŠ” ë³€ìˆ˜
        features = []
        
        # ê¸°ì¡´ padding ì ìš©ë˜ì§€ ì•Šì€ ë°ì´í„°ë¥¼ í•˜ë‚˜ì”© ë½‘ì•„ì„œ listë³€ìˆ˜ì— ì •ì˜
        for lst in tqdm(features):
          # max_len(ìµœëŒ€ í† í° ê¸¸ì´)ë³´ë‹¤ lstì˜ ê¸¸ì´ê°€ ì‘ìœ¼ë©´
          if len(lst) < max_len:
            # ê·¸ ì°¨ì´ë§Œí¼ 0ì„ ì¶”ê°€ : 0ì€ <pad>ì˜ indexê°’ì´ë¯€ë¡œ
            features.append(lst + [0] * (max_len - len(lst)))
          else:
            # ìµœëŒ€ í† í° ê¸¸ì´ì™€ ê°™ë‹¤.
            features(lst)
        ```
        
        ```python
        features = [ lst + [0] * (max_len - len(lst))  if len(lst) < max_len else lst for lst in tqdm(features) ]
        
        features = np.array(features)
        # (ì „ì²´ ë°ì´í„° ìˆ˜, ê° review data í† í° ìˆ˜)
        features.shape
        ```
        
4. Dataset
    
    ```python
    # target
    # í˜•ì‹ í†µì¼
    target = df_ko["label"].to_numpy()
    # ì°¨ì› í†µì¼
    target = target.reshape(-1,1)
    target.shape
    ```
    
    ```python
    class ReviewDataset(torch.utils.data.Dataset):
        def __init__(self,features,target=None):
            super().__init__()
            self.features = features # features
            self.target = target # target
    
        def __len__(self):
            return self.features.shape[0]
    
        # feature, target ê°’ ë°˜í™˜ -> torchí˜• ë°ì´í„° ì—¬ì•¼ í•œë‹¤.
        def __getitem__(self,idx):
            item = {}
            # ì¤‘ìš”!! ì •ìˆ˜ = embeddingí•  ë•Œ ë¬´ì¡°ê±´ longtensor ì—¬ì•¼ í•œë‹¤.
            item["x"] = torch.LongTensor(self.features[idx])
            if self.target is not None:
                # ì¤‘ìš”!! float = targetì€ ë¬´ì¡°ê±´ tensor
                item["y"] = torch.Tensor(self.target[idx])
            return item
    ```
    
    ```python
    # dataset classì˜ ì¸ìŠ¤í„´ìŠ¤
    dt = ReviewDataset(features, target)
    ```
    
5. DataLoader
    
    ```python
    dl = torch.utils.data.DataLoader(dt, batch_size=256, shuffle=False)
    len(dl) # ì „ì²´ ë°ì´í„°ë¥¼ / batch_size -> ë°ì´í„° ë¡œë”ì˜ ì‘ì—… ìˆ˜
    ```
    
    ```python
    batch = next(iter(dl))
    
    len(batch['x']), len(batch['y'])
    ```
    
6. CNN 1D Model
    
    ```python
    import torch
    
    class Conv1dModel(torch.nn.Module):
        def __init__(self,vocab_size,embedding_dim=128):
            super().__init__()
            # embedding : token(index ê°’)ì„ ì˜ë¯¸ìˆëŠ” ìˆ«ì ë¦¬ìŠ¤íŠ¸(vector)ë¡œ í‘œí˜„
            # vocab_size : ì „ì²´ í† í° ìˆ˜
            # embedding_dim : ê° í† í°ì„ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ë²¡í„° í¬ê¸°
            self.emb_layer = torch.nn.Embedding(vocab_size,embedding_dim)
            self.seq = torch.nn.Sequential(
                # CNN 1D
                torch.nn.Conv1d(in_channels=embedding_dim, out_channels=embedding_dim*2, kernel_size=3),
                torch.nn.ReLU(),
                torch.nn.MaxPool1d(2),
                torch.nn.Conv1d(in_channels=embedding_dim*2,out_channels=embedding_dim*4,kernel_size=3),
                torch.nn.ReLU(),
                torch.nn.MaxPool1d(2),
                torch.nn.AdaptiveAvgPool1d(1),
                # FC
                torch.nn.Flatten(),
                torch.nn.Linear(embedding_dim*4, 1)
            )
    
        def forward(self,x): # x (ë°°ì¹˜ í¬ê¸°, ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´)
            emb_out = self.emb_layer(x) # emb_out(ë°°ì¹˜ í¬ê¸°, ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´, ì„ë² ë”© ì•„ì›ƒí’‹ í¬ê¸°)
            emb_out_premute = emb_out.permute(0,2,1) # emb_out_premute(ë°°ì¹˜ í¬ê¸°, ì„ë² ë”© ì•„ì›ƒí’‹ í¬ê¸°, ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´)
            seq_out = self.seq(emb_out_premute) # seq_out(ë°°ì¹˜ í¬ê¸°, 1)
            return  seq_out
    ```
    
    ```python
    model = Conv1dModel(len(vocab))
    model
    ```
    
7. Engine
    
    ```python
    # train ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ” í”„ë¡œì„¸ìŠ¤
    def train_loop(dataloader,model,loss_fn,optimizer,device): # epoch ë‹¨ìœ„ í•™ìŠµ í”„ë¡œì„¸ìŠ¤
        # epoch_lossë¥¼ ì´ˆê¸°í™”
        epoch_loss = 0
        # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“ˆ ë³€ê²½
        model.train()
        # batchë§Œí¼ í•™ìŠµì„ ì§„í–‰
        for batch in dataloader:
            # ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
            pred = model(batch["x"].to(device))
            # ëª¨ë¸ì˜ í•™ìŠµ ê²°ê³¼ í‰ê°€ (ì‹¤ì œ ê°’ì—ì„œ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´)
            loss = loss_fn(pred, batch["y"].to(device))
            # ëª¨ë¸ì˜ ì—­ì „íŒŒ(ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ loss ì°¨ì´ë§Œí¼ ì¡°ì ˆ)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            # batchë³„ í•™ìŠµì˜ loss ê°’ì„ ì¶”ê°€
            epoch_loss += loss.item()
        # í‰ê·  batchë³„ í•™ìŠµ loss ê°’ ê³„ì‚°
        # batchë³„ í•™ìŠµ loss í‰ê·  ê°’ ê³„ì‚°
        # í•™ìŠµ loss í‰ê·  ê°’ ê³„ì‚°
        # í•™ìŠµ loss ê°’
        epoch_loss /= len(dataloader)
    
        return epoch_loss
    ```
    
    ```python
    from sklearn.metrics import accuracy_score
    
    @torch.inference_mode()
    def test_loop(dataloader,model,loss_fn,device):
        epoch_loss = 0
        total_acc = 0
        model.eval()
    
        pred_list = []
        sig = torch.nn.Sigmoid()
    
        for batch in dataloader:
    
            pred = model(batch["x"].to(device)) # pred: ì˜ˆì¸¡ê°’(ìŒìˆ˜ ê°’ì„ ê°–ê³  ìˆë‹¤.)
    
            pred_ = sig(pred) # pred_ í™•ë¥ ê°’(ì–‘ì˜ ê°’ìœ¼ë¡œ ë³€ê²½), pytorch ê°ì²´
            pred_ = pred_.to("cpu").numpy() # pred_: numpy ê°ì²´ë¡œ ë³€ê²½
            pred_list.append(pred_) # íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ pred_(ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’)
    
            if batch.get("y") is not None:
                loss = loss_fn(pred, batch["y"].to(device))
                epoch_loss += loss.item()
    
                pred_ = (pred_ > 0.5).astype(int) # í™•ë¥ ê°’ì„ ì •ì˜(1, 0)
                total_acc += accuracy_score(batch["y"].to("cpu").numpy(), pred_)
    
        epoch_loss /= len(dataloader)
        total_acc /= len(dataloader)
    
        # epoch_pred = np.concatenate(pred_list)
        return epoch_loss , total_acc
    ```
    
8. Split
    
    ```python
    from sklearn.model_selection import train_test_split
    
    X_tr, X_te, y_tr, y_te = train_test_split(features, target, test_size=0.1, shuffle=True)
    len(X_tr), len(y_tr), len(X_te), len(y_te)
    ```
    
9. KFold
    
    ```python
    from sklearn.model_selection import KFold, StratifiedKFold
    
    SEED = 42
    n_splits = 5 # ëª‡ë²ˆ split í•´ì„œ í•™ìŠµí• ì§€
    
    # cross validation ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    # n_splits ë²ˆ í•™ìŠµí•˜ê² ë‹¤.
    cv = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)
    ```
    
10. í•™ìŠµ
    
    ```python
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    device
    ```
    
    ```python
    from torch.utils.data import DataLoader
    ```
    
    ```python
    best_score_list = []
    epochs = 100
    batch_size = 256 #128
    vocab_size = len(vocab)
    
    # ì´ì§„ë¶„ë¥˜ -> ê°ì„±ë¶„ë¥˜ì¼ ë•Œ ë§ì´ ì‚¬ìš©
    loss_fn = torch.nn.BCEWithLogitsLoss()
    # ëª¨ë¸ ìƒì„±
    # split í•  ë•Œë§ˆë‹¤ ëª¨ë¸ ìƒˆë¡œ ìƒì„±
    model = Conv1dModel(vocab_size).to(device)
    # optimizer ìƒì„±
    optimizer = torch.optim.Adam(model.parameters())
    
    # cv.split(X_tr)ë¥¼ ì´ìš©í•´ tri(í•™ìŠµìš© ë°ì´í„°), vai(ê²€ì¦ìš© ë°ì´í„°)ë¡œ ë‚˜ëˆ ì§
    # tri, vai -> index array
    for i,(tri, vai) in enumerate(cv.split(X_tr)):
        # dataset ìƒì„±
        train_dt = ReviewDataset(X_tr[tri],y_tr[tri])
        valid_dt = ReviewDataset(X_tr[vai],y_tr[vai])
        # dataloader ìƒì„±
        train_dl = DataLoader(train_dt, batch_size=batch_size, shuffle=True)
        valid_dl = DataLoader(valid_dt, batch_size=batch_size, shuffle=False)
    
        # ì´ˆê¸°ê°’
        best_score = 0
        patience = 0
    
        for epoch in tqdm(range(epochs)):
            # ëª¨ë¸ í•™ìŠµ
            train_loss = train_loop(train_dl, model, loss_fn, optimizer, device )
            # ëª¨ë¸ í‰ê°€
            valid_loss, score = test_loop(valid_dl, model, loss_fn, device  )
    
            patience += 1
            if best_score < score:
                print(f'best score: {train_loss, valid_loss, score}') # íŠ¸ë ˆì¸ ë¡œìŠ¤, ë²¨ë¦¬ë“œ ë¡œìŠ¤, ìŠ¤ì½”ì–´
                patience = 0 # ì´ˆê¸°ê°’
                best_score = score # ì—…ë°ì´íŠ¸!!
                # torch.save(model.state_dict(),f"model_{i}.pth")
    
            if patience == 20:
                break
    
        print(f"Fold ({i}), BEST ACC: {best_score}")
        best_score_list.append(best_score)
    ```
    
11. Test
    
    ```python
    test_dt = ReviewDataset(X_te,y_te)
    test_dl = DataLoader(test_dt, batch_size=batch_size,shuffle=False)
    
    loss , pred = test_loop(test_dl, model, loss_fn, device  )
    
    loss, pred
    ```
