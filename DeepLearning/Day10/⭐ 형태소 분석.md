- 토큰화 ⇒ 글자를 쪼갠다. ⇒ 의미있는 덩어리로 쪼갠다
- 형태소 분석 종류가 많다. ⇒ 분석기도 많다.
- 상황에 맞는 분석기를 사용하면 된다.
1. nltk (영어만)
    1. python 에서 가장 오래되고 유명한 자연어 라이브러리(한국어 미지원)
    
    ```python
    import nltk
    
    nltk.download("punkt")
    nltk.download("averaged_perceptron_tagger")
    ```
    
    ```python
    sentence = """
    At eight o'clock on Thursday morning
    Arthur didn't feel very good."""
    
    tokens = nltk.word_tokenize(sentence)
    tokens
    '''
    ['At',
     'eight',
     "o'clock",
     'on',
     'Thursday',
     'morning',
     'Arthur',
     'did',
     "n't",
     'feel',
     'very',
     'good',
     '.']
    '''
    ```
    
    - 토큰이 뜻하는 정보 (품사 표시)
    
    ```python
    tagged = nltk.pos_tag(tokens)
    '''
    [('At', 'IN'),
     ('eight', 'CD'),
     ("o'clock", 'NN'),
     ('on', 'IN'),
     ('Thursday', 'NNP'),
     ('morning', 'NN'),
     ('Arthur', 'NNP'),
     ('did', 'VBD'),
     ("n't", 'RB'),
     ('feel', 'VB'),
     ('very', 'RB'),
     ('good', 'JJ'),
     ('.', '.')]
    '''
    ```
    
    - 명사(NN / NNP), 동사(VB / VBD)
    
    ```python
    lst = []
    # token : 글자, pos : 품사
    for token, pos in tagged:
      if pos.startswith("N") or pos.startswith("V"): # N: 명사, V: 동사
        lst.append(token)
    
    # 대부분 한줄로 코드를 작성한다.
    [ token for token, pos in tagged if pos.startswith("N") or pos.startswith("V") ]
    ```
    
2. spacy (한국, 영어, 일본, 중국어 ..)
    1. python 기반 오픈소스 라이브러리
    
    ```python
    pip install spacy
    python -m spacy download en
    python -m spacy download ko_core_news_sm
    ```
    
    - 영어
    
    ```python
    import spacy
    from spacy.lang.en.examples import sentences
    ```
    
    ```python
    # 모델 객체 생성
    nlp = spacy.load("en_core_web_sm")
    # 문장 모델 적용(예측)
    # 예제 문장 형태소 분석 (spacy 토큰 객체)
    doc = nlp(sentences[0])
    
    print(doc.text)
    print('-'*80)
    print("단어","원형","품사","태그", "의존성", "모양", "알파벳", "금칙어",sep="\t")
    for token in doc:
        print(
            token.text # 단어
            , token.lemma_ # 원형
            , token.pos_ # 품사
            , token.tag_ # 태그
            , token.dep_ # 의존성
            , token.shape_ # 모양
            , token.is_alpha # 알파벳
            , token.is_stop # 금칙어
            , sep='\t')
    ```
    
    - 한국어
    
    ```python
    import locale
    
    def getpreferredencoding(do_setlocale = True):
        return "UTF-8"
    
    locale.getpreferredencoding = getpreferredencoding
    ```
    
    ```python
    !python -m spacy download ko_core_news_sm
    ```
    
    ```python
    import spacy
    from spacy.lang.ko.examples import sentences
    ```
    
    ```python
    # 데이터 로드
    nlp = spacy.load("ko_core_news_sm")
    # 토큰화
    doc = nlp(sentences[0])
    
    print(doc.text)
    print('-'*80)
    print("단어","원형","품사","태그", "의존성", "모양", "알파벳", "금칙어",sep="\t")
    for token in doc:
        print(
            token.text # 단어
            , token.lemma_ # 원형
            , token.pos_ # 품사
            , token.tag_ # 태그
            , token.dep_ # 의존성
            , token.shape_ # 모양
            , token.is_alpha # 알파벳
            , token.is_stop # 금칙어
            , sep='\t')
    
    # for token in doc:
    #     print(token.text, token.pos_, token.dep_)
    ```
    
3. Konlpy (한국어만)
    1. 여러 토큰화 모듈들을 묶어놓은 패키지
        1. Hannanum
        2. Kkma
        3. Komoran
        4. Mecab
            
            ```python
            from konlpy.tag import Mecab
            
            mec = Mecab()
            txt1 = "아버지가방에들어가신다."
            txt2 = "아버지가 방에 들어가신다."
            ```
            
            ```python
            mec.pos(txt1)
            '''
            [('아버지', 'NNG'),
             ('가', 'JKS'),
             ('방', 'NNG'),
             ('에', 'JKB'),
             ('들어가', 'VV'),
             ('신다', 'EP+EF'),
             ('.', 'SF')]
            '''
            mec.pos(txt2)
            '''
            [('아버지', 'NNG'),
             ('가', 'JKS'),
             ('방', 'NNG'),
             ('에', 'JKB'),
             ('들어가', 'VV'),
             ('신다', 'EP+EF'),
             ('.', 'SF')]
            '''
            ```
            
            ```python
            [ token[0] for token in mec.pos(txt1) if token[1][0] in "NVJ" ] # 명사, 동사, 조사
            # ['아버지', '가', '방', '에', '들어가']
            ```
            
        5. Open Korean Text(Twiiter)
            
            ```python
            !git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git # 다운로드
            !bash /content/Mecab-ko-for-Google-Colab/install_mecab-ko_on_colab_light_220429.sh # 소스를 실행
            ```
            
            ```python
            from konlpy.tag import Okt
            
            okt = Okt()
            txt1 = "아버지가방에들어가신다."
            txt2 = "아버지가 방에 들어가신다."
            ```
            
            ```python
            okt.pos(txt1)
            '''
            [('아버지', 'Noun'),
             ('가방', 'Noun'),
             ('에', 'Josa'),
             ('들어가신다', 'Verb'),
             ('.', 'Punctuation')]
            '''
            okt.pos(txt2)
            '''
            [('아버지', 'Noun'),
             ('가', 'Josa'),
             ('방', 'Noun'),
             ('에', 'Josa'),
             ('들어가신다', 'Verb'),
             ('.', 'Punctuation')]
            '''
            ```
            
            ```python
            [ token[0] for token in okt.pos(txt2) if token[1][0] in "NVJ" ] # 명사, 동사, 조사
            # ['아버지', '가', '방', '에', '들어가신다']
            ```
            
4. kiwi
    1. 최근 많이 사요 (2023년 기준)
    2. 여러 토큰화 결과를 반환해준다.
    3. kiwi는 불용어 지원
    
    ```python
    !pip install kiwipiepy
    ```
    
    ```python
    from kiwipiepy import Kiwi
    
    kiwi = Kiwi()
    txt1 = "아버지가방에들어가신다."
    txt2 = "아버지가 방에 들어가신다."
    ```
    
    ```python
    # 여러개의 토큰 방법을 표현
    result1 = kiwi.analyze(txt1,top_n=3)
    '''
    [([Token(form='아버지', tag='NNG', start=0, len=3),
       Token(form='가', tag='JKS', start=3, len=1),
       Token(form='방', tag='NNG', start=4, len=1),
       Token(form='에', tag='JKB', start=5, len=1),
       Token(form='들어가', tag='VV', start=6, len=3),
       Token(form='시', tag='EP', start=9, len=1),
       Token(form='ᆫ다', tag='EF', start=9, len=2),
       Token(form='.', tag='SF', start=11, len=1)],
      -28.60936164855957),
     ([Token(form='아버지', tag='NNG', start=0, len=3),
       Token(form='가방', tag='NNG', start=3, len=2),
       Token(form='에', tag='JKB', start=5, len=1),
       Token(form='들어가', tag='VV', start=6, len=3),
       Token(form='시', tag='EP', start=9, len=1),
       Token(form='ᆫ다', tag='EF', start=9, len=2),
       Token(form='.', tag='SF', start=11, len=1)],
      -31.23612403869629),
     ([Token(form='아버지', tag='NNG', start=0, len=3),
       Token(form='가', tag='JKS', start=3, len=1),
       Token(form='방', tag='NNG', start=4, len=1),
       Token(form='에', tag='JKB', start=5, len=1),
       Token(form='들어가', tag='VV', start=6, len=3),
       Token(form='어', tag='EC', start=8, len=1),
       Token(form='신', tag='NNG', start=9, len=1),
       Token(form='다.', tag='SB', start=10, len=2)],
      -57.53723907470703)]
    '''
    ```
    
    ```python
    [ token.form for token in  result1[0][0]]
    # ['아버지', '가', '방', '에', '들어가', '시', 'ᆫ다', '.']
    ```
    
    - 불용어 적용
    
    ```python
    from kiwipiepy.utils import Stopwords
    stopwords =  Stopwords()
    result = kiwi.tokenize(txt_list, stopwords=stopwords) # 토큰화 & 불용어
    
    for tokens in result:
        print(tokens)
    ```
    
    ```python
    # 불용어 리스트
    stopwords =  Stopwords()
    stopwords.stopwords
    
    # Key - value 형태로 추가해 주어야 한다.
    # Key - 품사
    stopwords.add("NLP")
    stopwords.stopwords
    
    # 불용어 제거
    stopwords.remove(["NLP"])
    stopwords.stopwords
    ```
    

<aside>
💡 Reference

</aside>

- nltk
    - https://www.nltk.org/
- spacy
    - https://spacy.io/
- Konlpy
    - https://konlpy-ko.readthedocs.io/ko/v0.4.3/
- kiwi
    - [https://github.com/bab2min/Kiwi#품사-태그](https://github.com/bab2min/Kiwi#%ED%92%88%EC%82%AC-%ED%83%9C%EA%B7%B8)
