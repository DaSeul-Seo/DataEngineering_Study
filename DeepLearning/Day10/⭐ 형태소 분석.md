- í† í°í™” â‡’ ê¸€ìë¥¼ ìª¼ê° ë‹¤. â‡’ ì˜ë¯¸ìˆëŠ” ë©ì–´ë¦¬ë¡œ ìª¼ê° ë‹¤
- í˜•íƒœì†Œ ë¶„ì„ ì¢…ë¥˜ê°€ ë§ë‹¤. â‡’ ë¶„ì„ê¸°ë„ ë§ë‹¤.
- ìƒí™©ì— ë§ëŠ” ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ë©´ ëœë‹¤.
1. nltk (ì˜ì–´ë§Œ)
    1. python ì—ì„œ ê°€ì¥ ì˜¤ë˜ë˜ê³  ìœ ëª…í•œ ìì—°ì–´ ë¼ì´ë¸ŒëŸ¬ë¦¬(í•œêµ­ì–´ ë¯¸ì§€ì›)
    
    ```python
    import nltk
    
    nltk.download("punkt")
    nltk.download("averaged_perceptron_tagger")
    ```
    
    ```python
    sentence = """
    At eight o'clock on Thursday morning
    Arthur didn't feel very good."""
    
    tokens = nltk.word_tokenize(sentence)
    tokens
    '''
    ['At',
     'eight',
     "o'clock",
     'on',
     'Thursday',
     'morning',
     'Arthur',
     'did',
     "n't",
     'feel',
     'very',
     'good',
     '.']
    '''
    ```
    
    - í† í°ì´ ëœ»í•˜ëŠ” ì •ë³´ (í’ˆì‚¬ í‘œì‹œ)
    
    ```python
    tagged = nltk.pos_tag(tokens)
    '''
    [('At', 'IN'),
     ('eight', 'CD'),
     ("o'clock", 'NN'),
     ('on', 'IN'),
     ('Thursday', 'NNP'),
     ('morning', 'NN'),
     ('Arthur', 'NNP'),
     ('did', 'VBD'),
     ("n't", 'RB'),
     ('feel', 'VB'),
     ('very', 'RB'),
     ('good', 'JJ'),
     ('.', '.')]
    '''
    ```
    
    - ëª…ì‚¬(NN / NNP), ë™ì‚¬(VB / VBD)
    
    ```python
    lst = []
    # token : ê¸€ì, pos : í’ˆì‚¬
    for token, pos in tagged:
      if pos.startswith("N") or pos.startswith("V"): # N: ëª…ì‚¬, V: ë™ì‚¬
        lst.append(token)
    
    # ëŒ€ë¶€ë¶„ í•œì¤„ë¡œ ì½”ë“œë¥¼ ì‘ì„±í•œë‹¤.
    [ token for token, pos in tagged if pos.startswith("N") or pos.startswith("V") ]
    ```
    
2. spacy (í•œêµ­, ì˜ì–´, ì¼ë³¸, ì¤‘êµ­ì–´ ..)
    1. python ê¸°ë°˜ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬
    
    ```python
    pip install spacy
    python -m spacy download en
    python -m spacy download ko_core_news_sm
    ```
    
    - ì˜ì–´
    
    ```python
    import spacy
    from spacy.lang.en.examples import sentences
    ```
    
    ```python
    # ëª¨ë¸ ê°ì²´ ìƒì„±
    nlp = spacy.load("en_core_web_sm")
    # ë¬¸ì¥ ëª¨ë¸ ì ìš©(ì˜ˆì¸¡)
    # ì˜ˆì œ ë¬¸ì¥ í˜•íƒœì†Œ ë¶„ì„ (spacy í† í° ê°ì²´)
    doc = nlp(sentences[0])
    
    print(doc.text)
    print('-'*80)
    print("ë‹¨ì–´","ì›í˜•","í’ˆì‚¬","íƒœê·¸", "ì˜ì¡´ì„±", "ëª¨ì–‘", "ì•ŒíŒŒë²³", "ê¸ˆì¹™ì–´",sep="\t")
    for token in doc:
        print(
            token.text # ë‹¨ì–´
            , token.lemma_ # ì›í˜•
            , token.pos_ # í’ˆì‚¬
            , token.tag_ # íƒœê·¸
            , token.dep_ # ì˜ì¡´ì„±
            , token.shape_ # ëª¨ì–‘
            , token.is_alpha # ì•ŒíŒŒë²³
            , token.is_stop # ê¸ˆì¹™ì–´
            , sep='\t')
    ```
    
    - í•œêµ­ì–´
    
    ```python
    import locale
    
    def getpreferredencoding(do_setlocale = True):
        return "UTF-8"
    
    locale.getpreferredencoding = getpreferredencoding
    ```
    
    ```python
    !python -m spacy download ko_core_news_sm
    ```
    
    ```python
    import spacy
    from spacy.lang.ko.examples import sentences
    ```
    
    ```python
    # ë°ì´í„° ë¡œë“œ
    nlp = spacy.load("ko_core_news_sm")
    # í† í°í™”
    doc = nlp(sentences[0])
    
    print(doc.text)
    print('-'*80)
    print("ë‹¨ì–´","ì›í˜•","í’ˆì‚¬","íƒœê·¸", "ì˜ì¡´ì„±", "ëª¨ì–‘", "ì•ŒíŒŒë²³", "ê¸ˆì¹™ì–´",sep="\t")
    for token in doc:
        print(
            token.text # ë‹¨ì–´
            , token.lemma_ # ì›í˜•
            , token.pos_ # í’ˆì‚¬
            , token.tag_ # íƒœê·¸
            , token.dep_ # ì˜ì¡´ì„±
            , token.shape_ # ëª¨ì–‘
            , token.is_alpha # ì•ŒíŒŒë²³
            , token.is_stop # ê¸ˆì¹™ì–´
            , sep='\t')
    
    # for token in doc:
    #     print(token.text, token.pos_, token.dep_)
    ```
    
3. Konlpy (í•œêµ­ì–´ë§Œ)
    1. ì—¬ëŸ¬ í† í°í™” ëª¨ë“ˆë“¤ì„ ë¬¶ì–´ë†“ì€ íŒ¨í‚¤ì§€
        1. Hannanum
        2. Kkma
        3. Komoran
        4. Mecab
            
            ```python
            from konlpy.tag import Mecab
            
            mec = Mecab()
            txt1 = "ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤."
            txt2 = "ì•„ë²„ì§€ê°€ ë°©ì— ë“¤ì–´ê°€ì‹ ë‹¤."
            ```
            
            ```python
            mec.pos(txt1)
            '''
            [('ì•„ë²„ì§€', 'NNG'),
             ('ê°€', 'JKS'),
             ('ë°©', 'NNG'),
             ('ì—', 'JKB'),
             ('ë“¤ì–´ê°€', 'VV'),
             ('ì‹ ë‹¤', 'EP+EF'),
             ('.', 'SF')]
            '''
            mec.pos(txt2)
            '''
            [('ì•„ë²„ì§€', 'NNG'),
             ('ê°€', 'JKS'),
             ('ë°©', 'NNG'),
             ('ì—', 'JKB'),
             ('ë“¤ì–´ê°€', 'VV'),
             ('ì‹ ë‹¤', 'EP+EF'),
             ('.', 'SF')]
            '''
            ```
            
            ```python
            [ token[0] for token in mec.pos(txt1) if token[1][0] in "NVJ" ] # ëª…ì‚¬, ë™ì‚¬, ì¡°ì‚¬
            # ['ì•„ë²„ì§€', 'ê°€', 'ë°©', 'ì—', 'ë“¤ì–´ê°€']
            ```
            
        5. Open Korean Text(Twiiter)
            
            ```python
            !git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git # ë‹¤ìš´ë¡œë“œ
            !bash /content/Mecab-ko-for-Google-Colab/install_mecab-ko_on_colab_light_220429.sh # ì†ŒìŠ¤ë¥¼ ì‹¤í–‰
            ```
            
            ```python
            from konlpy.tag import Okt
            
            okt = Okt()
            txt1 = "ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤."
            txt2 = "ì•„ë²„ì§€ê°€ ë°©ì— ë“¤ì–´ê°€ì‹ ë‹¤."
            ```
            
            ```python
            okt.pos(txt1)
            '''
            [('ì•„ë²„ì§€', 'Noun'),
             ('ê°€ë°©', 'Noun'),
             ('ì—', 'Josa'),
             ('ë“¤ì–´ê°€ì‹ ë‹¤', 'Verb'),
             ('.', 'Punctuation')]
            '''
            okt.pos(txt2)
            '''
            [('ì•„ë²„ì§€', 'Noun'),
             ('ê°€', 'Josa'),
             ('ë°©', 'Noun'),
             ('ì—', 'Josa'),
             ('ë“¤ì–´ê°€ì‹ ë‹¤', 'Verb'),
             ('.', 'Punctuation')]
            '''
            ```
            
            ```python
            [ token[0] for token in okt.pos(txt2) if token[1][0] in "NVJ" ] # ëª…ì‚¬, ë™ì‚¬, ì¡°ì‚¬
            # ['ì•„ë²„ì§€', 'ê°€', 'ë°©', 'ì—', 'ë“¤ì–´ê°€ì‹ ë‹¤']
            ```
            
4. kiwi
    1. ìµœê·¼ ë§ì´ ì‚¬ìš” (2023ë…„ ê¸°ì¤€)
    2. ì—¬ëŸ¬ í† í°í™” ê²°ê³¼ë¥¼ ë°˜í™˜í•´ì¤€ë‹¤.
    3. kiwiëŠ” ë¶ˆìš©ì–´ ì§€ì›
    
    ```python
    !pip install kiwipiepy
    ```
    
    ```python
    from kiwipiepy import Kiwi
    
    kiwi = Kiwi()
    txt1 = "ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤."
    txt2 = "ì•„ë²„ì§€ê°€ ë°©ì— ë“¤ì–´ê°€ì‹ ë‹¤."
    ```
    
    ```python
    # ì—¬ëŸ¬ê°œì˜ í† í° ë°©ë²•ì„ í‘œí˜„
    result1 = kiwi.analyze(txt1,top_n=3)
    '''
    [([Token(form='ì•„ë²„ì§€', tag='NNG', start=0, len=3),
       Token(form='ê°€', tag='JKS', start=3, len=1),
       Token(form='ë°©', tag='NNG', start=4, len=1),
       Token(form='ì—', tag='JKB', start=5, len=1),
       Token(form='ë“¤ì–´ê°€', tag='VV', start=6, len=3),
       Token(form='ì‹œ', tag='EP', start=9, len=1),
       Token(form='á†«ë‹¤', tag='EF', start=9, len=2),
       Token(form='.', tag='SF', start=11, len=1)],
      -28.60936164855957),
     ([Token(form='ì•„ë²„ì§€', tag='NNG', start=0, len=3),
       Token(form='ê°€ë°©', tag='NNG', start=3, len=2),
       Token(form='ì—', tag='JKB', start=5, len=1),
       Token(form='ë“¤ì–´ê°€', tag='VV', start=6, len=3),
       Token(form='ì‹œ', tag='EP', start=9, len=1),
       Token(form='á†«ë‹¤', tag='EF', start=9, len=2),
       Token(form='.', tag='SF', start=11, len=1)],
      -31.23612403869629),
     ([Token(form='ì•„ë²„ì§€', tag='NNG', start=0, len=3),
       Token(form='ê°€', tag='JKS', start=3, len=1),
       Token(form='ë°©', tag='NNG', start=4, len=1),
       Token(form='ì—', tag='JKB', start=5, len=1),
       Token(form='ë“¤ì–´ê°€', tag='VV', start=6, len=3),
       Token(form='ì–´', tag='EC', start=8, len=1),
       Token(form='ì‹ ', tag='NNG', start=9, len=1),
       Token(form='ë‹¤.', tag='SB', start=10, len=2)],
      -57.53723907470703)]
    '''
    ```
    
    ```python
    [ token.form for token in  result1[0][0]]
    # ['ì•„ë²„ì§€', 'ê°€', 'ë°©', 'ì—', 'ë“¤ì–´ê°€', 'ì‹œ', 'á†«ë‹¤', '.']
    ```
    
    - ë¶ˆìš©ì–´ ì ìš©
    
    ```python
    from kiwipiepy.utils import Stopwords
    stopwords =  Stopwords()
    result = kiwi.tokenize(txt_list, stopwords=stopwords) # í† í°í™” & ë¶ˆìš©ì–´
    
    for tokens in result:
        print(tokens)
    ```
    
    ```python
    # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
    stopwords =  Stopwords()
    stopwords.stopwords
    
    # Key - value í˜•íƒœë¡œ ì¶”ê°€í•´ ì£¼ì–´ì•¼ í•œë‹¤.
    # Key - í’ˆì‚¬
    stopwords.add("NLP")
    stopwords.stopwords
    
    # ë¶ˆìš©ì–´ ì œê±°
    stopwords.remove(["NLP"])
    stopwords.stopwords
    ```
    

<aside>
ğŸ’¡ Reference

</aside>

- nltk
    - https://www.nltk.org/
- spacy
    - https://spacy.io/
- Konlpy
    - https://konlpy-ko.readthedocs.io/ko/v0.4.3/
- kiwi
    - [https://github.com/bab2min/Kiwi#í’ˆì‚¬-íƒœê·¸](https://github.com/bab2min/Kiwi#%ED%92%88%EC%82%AC-%ED%83%9C%EA%B7%B8)
