### torchvision Module

- torchvision.datasets
    - dataset ê´€ë ¨
- torchvision.models
    - í•™ìŠµ ëª¨ë¸ ê´€ë ¨
- torchvision.transforms
    - ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ê´€ë ¨
- torch.utils.data.DataLoader

â­

```python
import os, random
import numpy as np
import pandas as pd

# SEED = 42

# ëª¨ë“  seedë¥¼ ê³ ì •
def reset_seeds(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)    # íŒŒì´ì¬ í™˜ê²½ë³€ìˆ˜ ì‹œë“œ ê³ ì •
    np.random.seed(seed)
    torch.manual_seed(seed) # cpu ì—°ì‚° ë¬´ìž‘ìœ„ ê³ ì •
    torch.cuda.manual_seed(seed) # gpu ì—°ì‚° ë¬´ìž‘ìœ„ ê³ ì •
    torch.backends.cudnn.deterministic = True  # cuda ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ Deterministic(ê²°ì •ë¡ ì )ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê¸° (ì˜ˆì¸¡ì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„± ì œê±° )

reset_seeds()
# ì…€ ë‹¨ìœ„ë¡œ SEEDë¥¼ ê³ ì •í•´ì•¼ í•œë‹¤.
```

### CNN êµ¬ì¡°

![1](https://github.com/DaSeul-Seo/DataEngineering_Study/assets/67898022/c58f9154-b16e-495a-9707-28a2a2357e18)

1. Convolution Layer
    1. í•œ ìž¥ì˜ ì´ë¯¸ì§€ë¥¼ nê°œì˜ ì´ë¯¸ì§€ë¡œ ë¶ˆë¦°ë‹¤.
    2. ê°ê°ì˜ ì´ë¯¸ì§€ì— ëŒ€í•œ íŠ¹ì§•ë“¤ì„ í•œ ìž¥ í•œ ìž¥ ë§Œë“ ë‹¤.
        1. ì‚¬ëžŒë“¤ì˜ ë‰´ëŸ°ë“¤ì€ ìˆ˜í‰ë§Œ ì¸ì§€, ëŒ€ê°ì„ ë§Œ ì¸ì§€, ê°ê°ì˜ íŠ¹ì§•ë“¤ì„ ê¸°ì–µí•œë‹¤ëŠ” ì„¤ì´ ìžˆë‹¤.
        2. Convolution Layerë„ ê° íŠ¹ì§•ë“¤ì„ ì´ìš©í•´ íŠ¹ì„±ì„ ë½‘ì„ ìˆ˜ ìžˆë‹¤. (ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°)
    3. featureë¥¼ ë½‘ëŠ”ë‹¤. (MLì—ì„œ featureë¥¼ ë½‘ëŠ” ì—­í™œê³¼ ë¹„ìŠ·)
    4. ì´ë¯¸ì§€ í¬ê¸° ë³€í™” X, ì´ë¯¸ì§€ ìž¥ìˆ˜ ì¦ê°€
2. Max Pooling Layer
    1. í•˜ë‚˜í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ ì¤„ì¸ë‹¤.
    2. ê°ê°ì˜ íŠ¹ì„±ë“¤ì„ ë½‘ìœ¼ë©´ ì •ë³´ë“¤ì´ ì¼ë¶€ë§Œ ë“¤ì–´ê°€ ìžˆê¸°ì— (ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°) ëª¨ë¸ì´ í•™ìŠµí•˜ê¸°ì—ëŠ” ë¹„íš¨ìœ¨ì ì´ë‹¤.
    3. í•™ìŠµì„ íš¨ìœ¨ì ìœ¼ë¡œ í•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ ì••ì¶•í•œë‹¤. (íŠ¹ì„±ë§Œ ì¡´ìž¬í•˜ê²Œ)
    4. ì´ë¯¸ì§€ì˜ ìž¥ìˆ˜ ê³ ì •, ì´ë¯¸ì§€ í¬ê¸° ë³€í™” O
3. Convolution Layer, Max Pooling Layer ë°˜ë³µ
    1. ê°ê° íŠ¹ì„±ë„ ì¦ê°€, ì••ì¶•ë„ ì¦ê°€
4. Flatten Layer
    1. í•™ìŠµ
- ìž¥ì 
    - Convolution Layer, Max Pooling Layer ì´ë¯¸ì§€ íŠ¹ì„±ì„ ì¶”ì¶œ
    - ì—°ì‚°ëŸ‰ì´ ì ë‹¤

### CNN ì½”ë“œ

1. Convolution Layer
    1. nn.Conv2d(in_channels, out_channels, kernel_size)
    2. (1, 2)stride = ìˆ˜ì— ë”°ë¼ ë“¬ì„±ë“¬ì„± ì›€ì§ì—¬ ë°ì´í„° ì¶”ì¶œ
    3. padding = ê°€ìž¥ìžë¦¬ëŠ” 1ë²ˆë°–ì— í•™ìŠµì´ ì•ˆë˜ì—ˆê¸°ì— ê´€ì‹¬ ì˜ì—­ì´ ê°€ìž¥ìžë¦¬ì— ìžˆìœ¼ë©´ ì¶”ì¶œí•˜ê¸°ê°€ ì–´ë µë‹¤. ê·¸ëž˜ì„œ padding(ì™¸ë¶€)ì„ ì£¼ì–´ì„œ í•™ìŠµì„ ì œëŒ€ë¡œ í•  ìˆ˜ ìžˆê²Œ í•œë‹¤.
- Import Module

```python
# Import PyTorch
import torch
from torch import nn

from torch.utils.data import DataLoader

# Import torchvision
import torchvision
from torchvision import datasets, transforms
from torchvision.transforms import ToTensor

# Import matplotlib for visualization
import matplotlib.pyplot as plt

# Import tqdm for progress bar
from tqdm.auto import tqdm

# Check versions
# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11
print(f"PyTorch version: {torch.__version__}\ntorchvision version: {torchvision.__version__}")

# í•™ìŠµì— ì‚¬ìš©í•  CPUë‚˜ GPU ìž¥ì¹˜ë¥¼ ì–»ìŠµë‹ˆë‹¤.
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f'device: {device}')
```

```python
!pip install torchinfo
import torchinfo
```

```python
import os, random
import numpy as np
import pandas as pd

# SEED = 42

def reset_seeds(seed=42):
    # íŒŒì´ì¬
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)    # íŒŒì´ì¬ í™˜ê²½ë³€ìˆ˜ ì‹œë“œ ê³ ì •
    # numpy & pandas -> ë¨¸ì‹ ëŸ¬ë‹
    np.random.seed(seed)
    # pytorch -> ë”¥ëŸ¬ë‹
    torch.manual_seed(seed) # cpu ì—°ì‚° ë¬´ìž‘ìœ„ ê³ ì •
    torch.cuda.manual_seed(seed) # gpu ì—°ì‚° ë¬´ìž‘ìœ„ ê³ ì •
    torch.backends.cudnn.deterministic = True  # cuda ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ Deterministic(ê²°ì •ë¡ ì )ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê¸° (ì˜ˆì¸¡ì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„± ì œê±° )

reset_seeds()
```

- ì‚¬ìš© í•¨ìˆ˜ ì •ì˜
    - ëª¨ë¸ í‰ê°€
        
        ```python
        def eval_model(
            model: torch.nn.Module, # ë”¥ëŸ¬ë‹ ëª¨ë¸
            data_loader: torch.utils.data.DataLoader, # ë°ì´í„° ë¡œë”(ë°°ì¹˜)
            loss_fn: torch.nn.Module, # ë¡œìŠ¤í•¨ìˆ˜
            accuracy_fn, # í‰ê°€í•¨ìˆ˜
            device: torch.device = device # ë””ë°”ì´ìŠ¤(cpu, gpu)
        ):
        
            loss, acc = 0, 0 # test loss / test acc
            model.eval() # í‰ê°€ ëª¨ë“œ ì „í™”
            with torch.inference_mode(): # ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê³ ì •!!!!
                # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œë¥¼ í†µí•´ì„œ
                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ features, targets ë½‘ì•„ë‚¸ë‹¤...
                # X -> ë°°ì¹˜ ë‹¨ìœ„ë¡œ ëª¨ì¸ featuresë“¤....
                # y -> ë°°ì¹˜ ë‹¨ìœ„ë¡œ ëª¨ì¸ targets....
                for X, y in data_loader:
                    # ëª¨ë¸ì´ ì ìš©ëœ deviceì— feature, target ì ìš©....
                    X, y = X.to(device), y.to(device)
                    # ëª¨ë¸ì´ featureë¥¼ ì´ìš©í•˜ì—¬ pred(ì˜ˆì¸¡)....
                    y_pred = model(X)
                    # y_pred(ì˜ˆì¸¡)ê³¼ y(ì‹¤ì œ)ì˜ ì°¨ì´(loss) êµ¬í•˜ìž....
                    # loss_fn(y_pred, y) -> ì „ì²´ lossê°€ ì•„ë‹Œ, batch lossì´ë‹¤....
                    # loss += loss_fn(y_pred, y)
                    # -> ì „ì²´ ë°ì´í„°(epoch)ì˜ loss ??
                    # -> batch lossë“¤ì˜ í•©ì´ë‹¤....
                    loss += loss_fn(y_pred, y)
                    # ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ìž....
                    acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))
        
                # Scale loss and acc
                # loss /= len(data_loader)
                # -> ì „ì²´ batch loss í•© / batchì˜ ìˆ˜
                # -> batch í‰ê· ì˜ loss ê°’ì´ -> epoch loss ê°’ ì •ì˜í•  ìˆ˜ ìžˆë‹¤.!!!!
                loss /= len(data_loader)
                acc /= len(data_loader)
            return {"model_name": model.__class__.__name__, # only works when model was created with a class
                    "model_loss": loss.item(),
                    "model_acc": acc}
        ```
        
    - í‰ê°€ ì ìˆ˜ ê³„ì‚°
        
        ```python
        # Calculate accuracy (a classification metric)
        def accuracy_fn(y_true, y_pred):
            """Calculates accuracy between truth labels and predictions.
        
            Args:
                y_true (torch.Tensor): Truth labels for predictions.
                y_pred (torch.Tensor): Predictions to be compared to predictions.
        
            Returns:
                [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45
            """
            correct = torch.eq(y_true, y_pred).sum().item()
            acc = (correct / len(y_pred)) * 100
            return acc
        ```
        
    - ì†Œìš”ì‹œê°„
        
        ```python
        def print_train_time(start: float, end: float, device: torch.device = None):
            """Prints difference between start and end time.
        
            Args:
                start (float): Start time of computation (preferred in timeit format).
                end (float): End time of computation.
                device ([type], optional): Device that compute is running on. Defaults to None.
        
            Returns:
                float: time between start and end in seconds (higher is longer).
            """
            total_time = end - start
            print(f"Train time on {device}: {total_time:.3f} seconds")
            return total_time
        ```
        
    - Train Loop
        
        ```python
        # 1. ëª¨ë¸ í•™ìŠµ
        # 2. í•™ìŠµ ê²°ê³¼ì— loss ê°’
        # 3. lossê°’ì— ë”°ë¼ ì—­ì „íŒŒ...
        def train_step(model: torch.nn.Module,
                        data_loader: torch.utils.data.DataLoader,
                        loss_fn: torch.nn.Module,
                        optimizer: torch.optim.Optimizer,
                        accuracy_fn,
                        device: torch.device = device):
            # ì´ˆê¸°ê°’
            model.to(device).train()
            train_loss, train_acc = 0, 0
        
            # batch ë‹¨ìœ„ë¡œ í•™ìŠµ ì‹œìž‘!!!!
            # -> batch: ëª‡ë²ˆì§¸ í•™ìŠµì„ ì§„í–‰í•˜ëŠ”ì§€ì˜ ìˆ«ìž ...
            # -> X: features (batch, color, height, width)
            # -> y: target (batch)
            for batch, (X, y) in enumerate(data_loader):
                # ëª¨ë¸ê³¼ ê°™ì€ device(cpu or gpu) ì ìš©!!
                X, y = X.to(device), y.to(device)
        
                # 1. ëª¨ë¸ í•™ìŠµ
                # 1. Forward pass
                y_pred = model(X)
        
                # 2. í•™ìŠµ ê²°ê³¼ì— loss ê°’
                # 2. Calculate loss
                # loss ??? -> batch ë‹¨ìœ„ì˜ ì‹¤ì œê°’ì—ì„œ ì˜ˆì¸¡ê°’ì„ ëº€ ì°¨ì´....
                loss = loss_fn(y_pred, y)
                train_loss += loss # ì „ì²´ lossë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ batch ë‹¨ìœ„ lossë¥¼ í•©í•˜ê¸°..
                train_acc += accuracy_fn(y_true=y,
                                        y_pred=y_pred.argmax(dim=1)) # y_pred(batch, pred)
        
                # 3. lossê°’ì— ë”°ë¼ ì—­ì „íŒŒ...
                # 3. Optimizer zero grad
                optimizer.zero_grad()
        
                # 4. Loss backward
                loss.backward()
        
                # 5. Optimizer step
                optimizer.step()
        
            # train_loss -> batch ë‹¨ìœ„ì˜ lossì˜ í•©
            # len(data_loader) -> ì „ì²´ ë°ì´í„°ë¥¼ batch ë‹¨ìœ„ë¡œ ë‚˜ëˆˆê°’...
            # ë”°ë¼ì„œ `train_loss /= len(data_loader)`ëŠ”
            # batch ë‹¨ìœ„ì˜ loss í‰ê· ê°’ -> ì´ê²ƒì„ epoch ë‹¨ìœ„ì˜ lossë¡œ ì •ì˜ !!!
            train_loss /= len(data_loader) # train_loss = train_loss / len(data_loader)
            train_acc /= len(data_loader)
            print(f"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%")
        
            return train_loss, train_acc
        ```
        
    - Test Loop
        
        ```python
        def test_step(data_loader: torch.utils.data.DataLoader,
                        model: torch.nn.Module,
                        loss_fn: torch.nn.Module,
                        accuracy_fn,
                        device: torch.device = device):
            # ì´ˆê¸°í™”
            test_loss, test_acc = 0, 0
            model.eval() # put model in eval mode
        
            # ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ê³ ì •í•˜ëŠ” ì—­í• !!!!
            with torch.inference_mode():
                for X, y in data_loader:
                    # Send data to GPU
                    X, y = X.to(device), y.to(device)
        
                    # ì˜ˆì¸¡í•œë‹¤..
                    # 1. Forward pass
                    test_pred = model(X)
        
                    # ì˜ˆì¸¡ê°’ì„ í‰ê°€í•œë‹¤!!!
                    # 2. Calculate loss and accuracy
                    test_loss += loss_fn(test_pred, y)
                    test_acc += accuracy_fn(y_true=y,
                        y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels
                    )
        
                # Adjust metrics and print out
                test_loss /= len(data_loader)
                test_acc /= len(data_loader)
                print(f"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\n")
        
            return test_loss, test_acc
        ```
        
- Dataset

```python
# transforms -> ì´ë¯¸ì§€ ì „ì²˜ë¦¬ë¥¼ í•´ì£¼ëŠ” ëª¨ë“ˆ.....
# transforms.ToTensor() -> python ìˆ«ìžë¥¼ torchì˜ tensor í˜•ë³€í™˜....
# transforms.Normalize()
# -> ë°ì´í„°ë¥¼ ì •ê·œí™” í–ˆë‹¤....
# -> 3ì°¨ì› ì»¬ë¦¬ì´ë©´, 2D ë©”íŠ¸ë¦­ìŠ¤ ì°¨ì›ì´ê¸°...
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
```

```python
# pytorchì˜ dataset class ë§Œë“¤ë‹¤..!!!!
train_set = torchvision.datasets.CIFAR10(
    root='./data',        # ë°ì´í„° ì €ìž¥ ìœ„ì¹˜
    train=True,           # True: train set, False: test set
    download=True,       # ë‹¤ìš´ë¡œë“œ ì—¬ë¶€, (ì´ë¯¸ ë‹¤ìš´ë°›ì•˜ìœ¼ë©´ Falseë¡œ ì§€ì •)
    transform=transform   # ë°ì´í„° ì„ ì²˜ë¦¬ ìž‘ì—…
)

test_set = torchvision.datasets.CIFAR10(
    root='./data',
    train=False,
    download=True,
    transform=transform
)
```

```python
import requests
import zipfile
from pathlib import Path

# Setup path to data folder
data_path = Path("data/")
image_path = data_path / "pizza_steak_sushi" # data/pizza_steak_sushi

# If the image folder doesn't exist, download it and prepare it...
if image_path.is_dir():
    print(f"{image_path} directory exists.")
else:
    print(f"Did not find {image_path} directory, creating one...")
    image_path.mkdir(parents=True, exist_ok=True)

    # Download pizza, steak, sushi data
    with open(data_path / "pizza_steak_sushi.zip", "wb") as f:
        request = requests.get("https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip")
        print("Downloading pizza, steak, sushi data...")
        f.write(request.content)

    # Unzip pizza, steak, sushi data
    with zipfile.ZipFile(data_path / "pizza_steak_sushi.zip", "r") as zip_ref:
        print("Unzipping pizza, steak, sushi data...")
        zip_ref.extractall(image_path)
```

```python
import os

def walk_through_dir(dir_path):
  for dirpath, dirnames, filenames in os.walk(dir_path):
    print(f"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.")
```

```python
# Setup train and testing paths
train_dir = image_path / "train"
test_dir = image_path / "test"
```

```python
from torchvision import datasets

train_data = datasets.ImageFolder(root=train_dir, transform=simple_transform)
test_data = datasets.ImageFolder(root=test_dir, transform=simple_transform)
```

- DataLoader

```python
import os
from torch.utils.data import DataLoader

BATCH_SIZE = 32
NUM_WORKERS = os.cpu_count()
print(f"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.")

train_dataloader = DataLoader(train_data, 
                              batch_size=BATCH_SIZE,
                              shuffle=True, # shuffle
                              num_workers=NUM_WORKERS)

test_dataloader = DataLoader(test_data,
                             batch_size=BATCH_SIZE,
                             shuffle=False,
                             num_workers=NUM_WORKERS)

train_dataloader, test_dataloader
```

- Model

```python
# ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ìž!!!
# 1. nn.Moduleì„ ìƒì† ë°›ìž!!!
# 2. __init__(ìƒì„±í•¨ìˆ˜)ë¥¼ ë§Œë“¤ìž...
  # -> ë‹¤ë¥¸ í•„ìˆ˜ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•œ ë³€ìˆ˜ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜...
# 3. forward(í•™ìŠµì„ í•˜ëŠ” í•¨ìˆ˜)ë¥¼ ë§Œë“¤ìž...
  # -> ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì•Œê³ ë¦¬ì¦˜ì— ë§žì¶°ì„œ ì½”ë”©ì„ í•˜ìž...
  # -> CNN -> Conv2D + Pooling + Linear

class CNNModelV1(nn.Module): # 1. nn.Moduleì„ ìƒì† ë°›ìž!!!
    # 2. __init__(ìƒì„±í•¨ìˆ˜)ë¥¼ ë§Œë“¤ìž...
    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):
        # -> ë‹¤ë¥¸ í•„ìˆ˜ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•œ ë³€ìˆ˜ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜...
        super().__init__()
        # block_1 -> nn.Conv2d, nn.ReLU, nn.Conv2d, nn.ReLU, nn.MaxPool2d
        # -> nn.Conv2d * 2 + nn.MaxPool2d * 1
        self.block_1 = nn.Sequential(
            nn.Conv2d(in_channels=input_shape, # input_shape=3
	                    out_channels=hidden_units,
	                    kernel_size=3,
	                    stride=1,
	                    padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=hidden_units,
	                    out_channels=hidden_units,
	                    kernel_size=3,
	                    stride=1,
	                    padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2,
                         stride=2)
        )
        # block_2 -> nn.Conv2d, nn.ReLU, nn.Conv2d, nn.ReLU, nn.MaxPool2d
        # -> nn.Conv2d * 2 + nn.MaxPool2d * 1
        self.block_2 = nn.Sequential(
            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )
        # classifier -> nn.Flatten, nn.Linear
        self.classifier = nn.Sequential(
            # block_2.shape -> 1Dë³€ê²½...  (batch(=32), hidden_units, 16, 16)
            # -> [32, 10, 16, 16]
            nn.Flatten(),
            nn.Linear(in_features=hidden_units*16*16, # in_features = (batch(=32), hidden_units*16*16)
                      out_features=hidden_units*16*16),
            nn.ReLU(),
            nn.Linear(in_features=hidden_units*16*16, # in_features = (batch(=32), hidden_units*16*16)
                      out_features=output_shape) # output_shape = len(target)
        )

    def forward(self, x: torch.Tensor): # (batch(=32), color(=3), height(=64), width(=64))
        x1 = self.block_1(x)
        x2 = self.block_2(x1)
        x3 = self.classifier(x2)
        return x3
```

```python
cnn_model = CNNModelV1(input_shape=3,
									     hidden_units=10,
									     output_shape=len(train_data.classes)).to(device)

cnn_model
```

```python
# cnn model
torchinfo.summary(cnn_model,(32, 3, 64, 64), col_names=["kernel_size", "input_size", "output_size", "num_params"])
```

- Training

```python
# Setup loss and optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(params=cnn_model.parameters(),
                            lr=0.1)
```

```python
SEED = 42
reset_seeds()

# Measure time
from timeit import default_timer as timer
train_time_start_model_2 = timer()

# Train and test model
epochs = 30
eopch_list = []
train_loss_list = []
test_loss_list = []
train_acc_list = []
test_acc_list = []

for epoch in tqdm(range(epochs)):
    print(f"Epoch: {epoch}\n---------")
    eopch_list.append(epoch)
    train_loss, train_acc = train_step(data_loader=train_dataloader,
															         model=cnn_model,
															         loss_fn=loss_fn,
															         optimizer=optimizer,
															         accuracy_fn=accuracy_fn,
															         device=device)
    train_loss_list.append(train_loss.detach().numpy())
    train_acc_list.append(train_acc)

    test_loss, test_acc = test_step(data_loader=test_dataloader,
														        model=cnn_model,
														        loss_fn=loss_fn,
														        accuracy_fn=accuracy_fn,
														        device=device)
    test_loss_list.append(test_loss.detach().numpy())
    test_acc_list.append(test_acc)

train_time_end_model_2 = timer()
total_train_time_cnn_model = print_train_time(start=train_time_start_model_2,
                                              end=train_time_end_model_2,
                                              device=device)
```

- loss ê·¸ëž˜í”„

```python
# Plot the loss curves
plt.plot(eopch_list, train_loss_list, label="Train loss")
plt.plot(eopch_list, test_loss_list, label="Test loss")
plt.title("Training and test loss curves")
plt.ylabel("Loss")
plt.xlabel("Epochs")
plt.legend();
```

- Confusion Matrix

```python
from tqdm.auto import tqdm

y_preds = []
cnn_model.eval()
with torch.inference_mode():
	# test ë°ì´í„° í•˜ë‚˜ì”© ê°€ì ¸ì˜¤ê¸° (feature, target)
  for X, y in tqdm(test_dataloader, desc = "Making predictions"):
		# ëª¨ë¸ê³¼ ë™ì¼í•œ í™˜ê²½ ì„¸íŒ…
    X, y = X.to(device), y.to(device)
		# ì˜ˆì¸¡
    y_logit = cnn_model(X)
		# ì˜ˆì¸¡ê°’ì„ í™•ë¥ ê°’ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ í›„ ìµœëŒ€ê°’ì˜ ë¼ë²¨ ë°˜í™˜
    y_pred = torch.softmax(y_logit, dim = 1).argmax(dim = 1)
		# ë¼ë²¨ ë¦¬ìŠ¤íŠ¸ì— ê°’ ì €ìž¥
    y_preds.append(y_pred.cpu())
  y_pred_tensor = torch.cat(y_preds)
```

```python
try:
  import torchmetrics, mlxtend
  print(f"mlxtend version: {mlxtend.__version__}")
  assert int(mlxtend.__version__.split(".")[1]) >= 19
except:
  !pip install -q torchmetrics -U mlxtend
  import torchmetrics, mlxtend
  print(f"mlxtend version: {mlxtend.__version__}")
```

```python
from torchmetrics import ConfusionMatrix
from mlxtend.plotting import plot_confusion_matrix

confmat = ConfusionMatrix(num_classes = len(train_data.classes), task = "multiclass")
confmat_tensor = confmat(preds = y_pred_tensor,
                         target = torch.tensor(test_data.targets))

fig, ax = plot_confusion_matrix(conf_mat = confmat_tensor.numpy(),
                                class_names = train_data.classes,
                                figsize = (10, 7))
```

<aside>
ðŸ’¡ Reference

</aside>

- https://pytorch.org/vision/stable/index.html
- CNN
    - https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
    - https://poloclub.github.io/cnn-explainer/
